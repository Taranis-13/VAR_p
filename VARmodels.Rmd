


---
title: "Modèles VAR"
author: "Henri BERTHET"
date: "28/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```

```{r, library, message=F, warning=F}
library("dse")
library("urca")
library("tsDyn")
library("stringr")
library("lubridate")
```
## Prologue

Mon but dans ce billet est de rappeler d'abord la définition d'un processus multivarié, d'en proposer une ou plusieurs simulations "maison" proche de la définition matricielle, puis d'appréhender deux fonctions de R "simulate" et VAR.sim pour en comprendre mieux le paramétrage.
Ensuite j'aborderai les test de stationnarité, puis de cointégration. Bonne lecture.
Petite remarque: afin d'éviter des "bug" de la fonction "Knit" il est préférable d'utiliser le package "remotes::install_github('yihui/knitr')"

## Processus Var(p)

On rappelle qu'un processus vectoriel est un VAR(p) s'il est défini par:
$$X_{t}=B_{0}+\sum_{i=1}^{p} B_{i} X_{t-i}+\varepsilon_{t}$$
où $X_{t}$ est un vecteur $(x_{1 t},x_{2 t},...,...,x_{k t})^{'}$  

On peut trouver un définition voisine:
$$AX_{t}=B_{0}+\sum_{i=1}^{p} B_{i} X_{t-i}+\varepsilon_{t}$$
Si A est inversible, on retrouve la première version en multipliant les deux membres de l'égalité par $A^{-1}$

Il est bon de noter que pour un VAR(1) $i\in[0;+\infty[$ , $X_{0}$ est la condition initiale, et $X_{1}$ est 
connu par le premier "choc" $\epsilon_{1}$. Nos simulations prendrons en général par défaut $X_{0}=0$ 

## Une fonction pédagogique 

Nous créons une fonction pour générer des processous VAR(p), non pas pour rivaliser avec les fonctions intégrées de R (VAR.sim ou simulate) mais pour mettre en évidence les calculs matriciels nécssaire à la génération.
Nous la nommerons var(), elle prendra en paramètres n le nombre d'observations du processus, la matrice A si nécessaire, B, p l'ordre du var (la fonction est encore en constructioin et fonctionne par défaut avec p=1) et le mode "const" ou "none" suivant qu'il y ait $B_{0}$ ou non.
Cette fonction ne permet (pour l'instant) que de générer des VAR(1) et des bruits blancs gaussiens non corrélés.

```{r, var}
var=function(n,A=diag(nrow(B)) , B,lag=1,mod=c("const","none","trend")){
  Ai=solve(A)
  C=ncol(B)
  N=nrow(B)
  vdat=matrix(0,N,n)
  start=1+lag
  v = matrix(rnorm(N*n),nrow = N,ncol=n,byrow = T)
  
  if(mod=="none") {
    vdat[,1:lag]=Ai%*%v[,1:lag] # initialisation
    for(i in start:n) {vdat[,i]=Ai%*%(B%*%vdat[,i-lag] +v[,i])} 
    return(t(vdat)) } else
      if(mod=="const") {
        vdat[,1:lag]=Ai%*%(v[,1:lag] + B[,1])    # initialisation 
        for(i in start:n) {vdat[,i]=Ai%*%(B[,2:C]%*%vdat[,i-lag] +v[,i]+B[,1])} 
        return(t(vdat)) } else
          if(mod=="trend") {
            vdat[,1:lag]=Ai%*%(v[,1:lag] + B[,1]+B[,2])    # initialisation
            for(i in start:n) {vdat[,i]=Ai%*%(B[,3:C]%*%vdat[,i-lag] +v[,i]+B[,1]+i*B[,2])} 
            return(t(vdat)) } 
}  

```

A titre d'exemple prenons le Var(1) suivant:
$$\left(\begin{array}{l}x_{t} \\y_{t}\end{array}\right)=\left(\begin{array}{ll}
0.7 & 0.2 \\0.2 & 0.7\end{array}\right)\left(\begin{array}{l}x_{t-1} \\y_{t-1}
\end{array}\right)+\left(\begin{array}{l}
\epsilon_{1_{t}} \\\epsilon_{2_{t}}\end{array}\right)$$

sur 100 réalisations, les $\epsilon_{i}$ des bruits blancs normaux centrés réduits indépendants.

```{r, exemple1}
set.seed(123456)
B1<-matrix(c(0.7, 0.2, 0.2, 0.7), 2)
vdat1=var(100,B=B1,lag=1,mod="none")
head(vdat1)
ts.plot(vdat1,type="l",col=c(1,2),main="Exemple 1")
```

Si on veut rajouter une constante, $\left(\begin{array}{ll}0.5\\0.8\end{array}\right)$


```{r, exemple2}
set.seed(123456)
B2<-matrix(c(0.5,0.7, 0.2, 0.8,0.2, 0.7),nrow= 2,byrow=T)
vdat2=var(100,B=B2,lag=1,mod="const")
head(vdat2)
ts.plot(vdat2,type="l",col=c(1,2),main="Exemple 1 avec constante")
```
Remarquons au passage qu'avoir ajouté une constante (0.5;0.8) dans le processus bivarié ne cause pas une translation de 0.5 ou 0.8 sur chaque variable (sauf pour la première valeur). Mais en simulant à deux reprises le même bruit blanc la différence entre les deux processus semble converger, mais ce cas particulier ne se trouve jamais dans la réalité.

## Fonction "simulate" de R

Cette fonction simule un VAR(p) dont les coefficients sont préalablement définis par VARMA(), tout ceci dans la librairie "dse".  
Pour saisir correctement les coefficients il faut écrire le processus de la façon suivante:  
$$Id.X_{t}-\sum_{i=1}^{p} B_{i} X_{t-i}=B_{0}+\varepsilon_{t}$$       
et saisir les coefficients de chaque matrice (sans oublier Id !!!) dans l'ordre L1C1, L2C1, L1C2, L2C2...dans une array de 2 sous matrices 2*2...!! .
On peut imposer une matrice B correspondant à un polynôme MA. S'il n'y a pas de composante MA autre que $\varepsilon_{t}$ B est l'identité.
Le vecteur constante correspond au paramètre TREND lorsqu'il est utilisé.


Nous allons retrouver les deux processus générés par notre fonction (à condition d'utiliser la même séquence de bruits blancs)

```{r, simulate2, message=FALSE,warning=FALSE}
Apoly= array(c(1,-0.7,0,-0.2,0,-0.2,1,-0.7),c(2,2,2))  
B = diag(2)
var1=ARMA(A=Apoly,B=B)  
set.seed( 123456)
w = matrix(rnorm(200),nrow = 100,ncol=2)
varsim1= simulate(var1,sampleT = 100,noise = w)  

vardat1=matrix(varsim1$output,nrow = 100,ncol=2)
head(vardat1)
colnames(vardat1)=c( "y1", "y2")
ts.plot(vardat1, type="l", col=c(1,2),main="Exemple 1 avec simulate sans constante")

TRD=c(0.5,0.8)
var11=ARMA(A=Apoly,B = B,TREND=TRD)
varsim11= simulate(var11,sampleT = 100,noise = w )
vardat11=matrix(varsim11$output,nrow = 100,ncol=2)
ts.plot(vardat11, type="l", col=c(1,2),main="Exemple 1 avec simulate avec constante")
```

L'écriture du processus sous la seconde forme permet de faire apparaître le "polynôme des retards". En effet on nomme L l'opérateur défini par $L(X_{t})=X_{t-1}$ (qui est linéaire ). Par voie de conséquence on aura $L^{p}(X_{t})=X_{t-p}$ , à ce compte là on peut écrire $$Id.X_{t}-\sum_{i=1}^{p} B_{i} X_{t-i}=B_{0}+\varepsilon_{t}=(Id-\sum_{i=1}^{p}B_{i}L^{i})X_{t}=B_{0}+\varepsilon_{t}$$  

Ce polynôme retard interviendra lorsque nous aborderons la stationnarité des processus.

### Exemple d'un VAR(2)

$$\left[\begin{array}{l}y_{1} \\y_{2}\end{array}\right]_{t}=\left[\begin{array}{c}2 \\6
\end{array}\right]+\left[\begin{array}{cc}
0.4 & 0.2 \\
-0.2 & -0.4
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2}
\end{array}\right]_{t-1}+\left[\begin{array}{cc}
-0.2 & 0.5 \\
-0.1 & 0.3
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2}
\end{array}\right]_{t-2}+\left[\begin{array}{c}
\varepsilon_{1} \\
\varepsilon_{2}
\end{array}\right]_{t}$$

Nous introduisons dans ce modèle une corrélation entre les bruits blancs définie par la matrice $\left[\begin{array}{cc}1 & 0.3 \\0.3 & 1\end{array}\right]$


```{r, simulate1,message=FALSE }
Apoly2= array(c(1.0,-0.4,0.2,0,0.2,0.1,0,-0.2,-0.5,1,0.4,-0.3),c(3,2,2))
B = diag (2)
C=matrix(c(1,0.3,0.3,1),2)
TRD=c(2,6)
var2=ARMA(A=Apoly2,B = B,TREND=TRD)
set.seed(123456) 
varsim2= simulate(var2,sampleT = 500,Cov=C )
vardat2=matrix(varsim2$output,nrow = 500,ncol=2)
colnames(vardat2)=c( "y1","y2")
plot.ts( vardat2,main ="VAR(2) avec simulate", xlab ="’’")

```


## Fonction VAR.sim de R

VAR.sim de la librairie tsDyn réalise également des simulations de modèles Var(p). La saisie des coefficients est assez proche de celle de notre fonction pédagogique, à savoir la matrice des coefficeients relative à l'écriture $$X_{t}=B_{0}+\sum_{i=1}^{p} B_{i} X_{t-i}+\varepsilon_{t}$$ ; lag=p pour l'ordre du Var, la présence de la constante ou non est paramétrée par include ="const" ou "none", le bruit blanc est "innov=". 
Voici ce que donne notre premier Var(1)  

```{r, VAR.sim1, , warning=FALSE,message=FALSE}

B1<-matrix(c(0.7, 0.2, 0.2, 0.7), 2)
set.seed(123456)
w = matrix(rnorm(200),nrow = 100,ncol=2)
var1<-VAR.sim(B=B1,lag=1,n=100,include="none",innov=w)
ts.plot(var1, type="l", col=c(1,2),main="Exemple 1 avec VAR.sim")

``` 

En introduisant la constante pour le Var(2)
```{r, VAR.sim2}
B2<-rbind(c(2, 0.4, 0.2,-0.2,0.5), c(6, -0.2, -0.4,-0.1,0.3))
C=matrix(c(1,0.3,0.3,1),2)
set.seed(123456)
var2 <- VAR.sim(B=B2,n=500, lag=2,include="const",varcov=C )
plot.ts( var2,main ="VAR(2) avec VAR.sim", xlab ="’’")
```

## Stationnarité

On rappelle qu'un processus est stationnaire (faiblement ou de second ordre) si et seulement si:
$$E\left[X_{t}\right]=\mu, \forall t \in \mathbb{Z}$$
$$E\left[X_{t} X_{t}^{\prime}\right]<\infty, \forall t \in \mathbb{Z}$$
$$E\left[\left(X_{t}-\boldsymbol{\mu}\right)\left(X_{t-h}-\boldsymbol{\mu}\right)^{\prime}\right]=\Gamma_{X}(h)=\Gamma_{X}^{\prime}(-h), \quad \forall t \in \mathbb{Z} \text { et } h=0,1, \ldots$$
La dernière ligne de la définition exprime que la matrice des covariances des composantes est indépendante de t.

Les termes de cette matrice sont $\gamma_{i j}(h)$ les fonctions d'autocorrélation des $y_{i}$ si $i=j$ et les fonctions d'autocorrélation croisées entre $y_{i}$ et $y_{j}$.  
$$\gamma_{i j}(h)=\operatorname{Cov}\left(y_{i t}, y_{j, t-h}\right), \forall i \forall j$$

Attention, cette définition est valable pour tous les types de processus. Pour un VAR(p) la stationnarité fera l'objet d'une propriété spécifique, qui évitera le recours à cette définition générale pas toujours facile à manipuler.

Il apparaît clairement que si le processus vectoriel est stationnaire les composantes le sont. Visuellemnt si on constate au moins une composante non stationnaire, le processus ne l'est pas (on disposera d'une batterie de tests). La réciproque est fausse, donc éviter de conclure hâtivement en voyant les graphiques de chaque composante. Voici un contrexemple:  
$$y_{t}=\left\{\begin{array}{lcc}
y_{1, t} & = & \varepsilon_{1, t} \\
y_{2, t} & = & \varepsilon_{2, t}+(-1)^{t} \varepsilon_{1, t}
\end{array}\right.$$
Il est clair que chaque composante est stationnaire:
Leurs espérances sont nulles, donc indépendantes de t et,
$$\gamma_{y_{1}}(h)=Cov(y_{1,t},y_{1,t-h})=E(y_{1,t}.y_{1,t-h})=E(\epsilon_{1,t}.\epsilon_{1,t-h})=\gamma_{\epsilon_{1}}(h) $$
qui ne dépend pas de t, puisque  $\epsilon_{1,t}$ est bruit blanc (donc stationnaire).
De même,
$$\operatorname{Cov}\left(y_{2, t+h}, y_{2, t}\right)=E\left[y_{2, t+h} y_{2, t}\right]=E\left[\left(\varepsilon_{2, t+h}+(-1)^{t+h} \varepsilon_{1, t+h}\right)\left(\varepsilon_{2, t}+(-1)^{t} \varepsilon_{1, t}\right)\right]=\gamma_{\varepsilon_{2}}(h)+(-1)^{h} \gamma_{\varepsilon_{1}}(h)$$
On peut constater visuellement la stationnarité des séries sur le graphique ci-dessous.

```{r, contrexemple}

set.seed(123456)
w = matrix(rnorm(200),nrow = 100,ncol=2)
y1=w[,1]
y2=w[,2]+rep(c(-1,1),50)*y1
vdat3=matrix(c(y1,y2),nrow=100)
plot.ts(vdat3,type="l",col=c(1,2))

```

Mais le processus bivarié n'est pas stationnaire...!
$$\operatorname{Cov}\left(y_{2, t+h}, y_{1, t}\right)=E\left[y_{2, t+h} y_{1, t}\right]=E\left[\left(\varepsilon_{2, t+h}+(-1)^{t+h} \varepsilon_{1, t+h}\right)\left( \varepsilon_{1, t}\right)\right]=(-1)^{t+h} \gamma_{\varepsilon_{1}}(h)$$
qui est un terme dépendant de t.
En terminant tous les calculs de la matrice des covariances on a: 
$$\Gamma_{h,t}=\left[\begin{array}{cc}
\gamma_{\varepsilon_{1}(h)} & (-1)^{t} \gamma_{\varepsilon_{1}(h)} \\
(-1)^{t+h} \gamma_{\varepsilon_{1}(h)} & \left.\gamma_{\varepsilon_{2}(h)}+(-1)^{h}\right) \gamma_{\varepsilon_{1}(h)}
\end{array}\right]$$
qui dépend de t.

# Stationnarité d'un VAR(p)

## Cas d'un VAR(1)

Quiconque a essyé de coder une série de type AR(1), (ie $X_{t}=aX_{t-1}+\epsilon_{t}$) s'est rendu compte que le processus est visiblement stationnaire pour |a|<1, ou est une marche aléatoire pour |a|=1 et "explosif" pour |a|>1 .
Ecrivons un tel processus sous la forme:
$$(1-aL)X_{t}=\epsilon_{t}$$  
avec L l'opératuer retard. Cet opérateur est une application linéaire de l'espace vectoriel des fonctions $X_{t}$ muni de la norme $||X||=\underset{t\in{R}}{sup}{(|X_{t}|)} $ 
qui est donc un espace normé complet (espace de Banach). Par voie de conséquence les apllications linéaires sont dotées aussi d'une norme.
$$||L||=\underset{||X||=1}{sup}(||L(X)||)$$
Pour notre opérateur retard L on a $||L||=1$ (facile à vérifier), par conséquent si |a|<1 on aura $||aL||<1$. Quelques connaissances de calcul différentiel nous permettent de conclure que l'opérateur $(1-aL)$ est inversible et que son inverse sera:
$$(1-aL)^{-1}=\sum_{j=0}^{+\infty}(aL)^{j}$$
Donc le processus $X_{t}$ serait défini par:
$$X_{t}=\sum_{j=0}^{+\infty}a^{j}\epsilon_{t-j}$$  
si cette série converge..!  
On sait déjà que $\sum_{j=0}^{+\infty}|a|^{j}$ est absolument convergente , puisqu |a|<1, et par conséquent que $\sum_{j=0}^{+\infty}({a^{2}})^{j}$ converge aussi. Le théorème de Kolmogorov nous assure alors la convergence presque partout de $X_{t}=\sum_{j=0}^{+\infty}a^{j}\epsilon_{t-j}$ les $\epsilon_{t-j}$ étant indépendants.  

Ce que nous venons d'étudier pour une série simple se transpose aisément au cas d'un VAR(1) multivarié. $X_{t}$ et $\epsilon_{t}$ sont des vecteurs se dimension N, et a devient une matrice A $N\times N$. Si on note encore L l'opérateur retard, le processus est caractérisé par:
$$(1-AL)X_{t}=\epsilon_{t}$$
La condition d'inversibilité est $||AL||<1$, donc $||A||<1$. Dans la théorie des espaces vectoriels normés la norme d'une matrice (pzrmi d'autres normes !) est le module de sa plus grande valeur propre. D'où le résultat:  
*Un processus Var(1) $(1-AL)X_{t}=\epsilon_{t}$ est stable si A a toutes ses valeurs propres en module strictement inférieures à 1*  

N.B: Le lecteur attentif aura remarqué que nous disons "stable" et non plus "stationnaire". 
En effet il existe des processus multivariés avec des valeurs propres de A supérieurs à 1 mais stationnaires faiblement (ou du second ordre).  

## Cas d'un VAR(p)  

On travaille maintenant sur un processus de K variables du type:  
$$y_{t}=\nu+A_{1} y_{t-1}+A_{2} y_{t-2}+\cdots+A_{p} y_{t-p}+u_{t}, \quad t \in \mathbb{Z}$$
On va, par un procédé analogue à celui qui nous permettait de transformer une équation différentielle d'ordre p en un sytème différentiel d'ordre 1, se rammener à un VAR(1).  
On pose: 
$$Y_{t}=\left[\begin{array}{c}
y_{t} \\
y_{t-1} \\
\vdots \\
y_{t-p+1}
\end{array}\right]$$
$$\mathbf{C}=\left[\begin{array}{c}
\nu \\
0 \\
\vdots \\
0
\end{array}\right]$$
$$\mathbf{U}_{t}=\left[\begin{array}{c}
u_{t} \\
0 \\
\vdots \\
0
\end{array}\right]$$
et
$$\mathbf{A}=\left[\begin{array}{ccccc}
A_{1} & A_{2} & \cdots & A_{p-1} & A_{p} \\
I_{K} & 0 & \ldots & 0 & 0 \\
0 & I_{K} & \ddots & 0 & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & \ldots & I_{K} & 0
\end{array}\right]$$

Attention $y_{t},\nu, u_{t}, 0$ sont déjà des vecteurs colonnes de dimension K; 
A est une matrice de dimension (K.p)² (matrice dite "par blocs") qui est appelée "MATRICE COMPAGNON du processus"
A ce prix on obtient une écriture équivalente:
$$Y_{t}=\mathbf{C}+\mathbf{A} Y_{t-1}+\mathbf{U}_{t}, t \in \mathbb{Z}$$
**CONCLUSION: Le processus est stable si les valeurs propres de la marice compagnon sont strictement inférieures à 1.**

### Exemple
Dans un premier temps nous créons une fonction qui retourne la matrice compagnon d'un VAR(p) simulé par VAR.sim.

```{r, fonction companion}
companion=function(B) {
N=nrow(B)
C=ncol(B)
if(floor(C/N)==C/N) { p=C/N ; BB=B} else # detection d'une constante
{ p=floor(C/N) 
BB=B[,2:C]  # si constante
}
return(rbind(BB,cbind(diag(N*(p-1)),matrix(0,nrow=N*(p-1),ncol=N))))
}

```

Appliquons la au VAR(2)

```{r, Matrice compagnon}
COMP=companion(B2)
print(COMP)
x=eigen(COMP,only.values=T)$values
print("Modules des valeurs propres")
print(sapply(x,abs),quote=F,digits=3)
```

Elles n'excèdent pas 0.7980 , le processus VAR(2) est donc stable.  

On a pu ici les calculer exactement, car le processus est "préfabriqué". Dans la réalité nous traiterons de processus ajustés sur des séries réelles, ce qui confèrera un caractère aléatoire à ces valeurs propres. Leur loi de probabilité étant déterminée ( Dickey et Fuller) on pourra tester l'hypothèse nulle "une au moins des valeurs propres a un module égal à 1" contre l'hypothèse alternative "tous les modules sont inférieurs à 1". 
  
Dans la littérature on lit souvent l'expression "racines unitaires" ("units roots"), de quoi s'agit-t-il?  
Lorsqu'on calcule les valeurs propres de A, la matrice compagnon, on résoud l'équation:
$$det(A-\lambda.Id)=0$$ 
Avec un peu d'algèbre linéaire et une manipulation de matrices par blocs on montre que l'équation ci-dessus est équivalante à $$\operatorname{det}\left(\lambda^{p}.Id-A_{1} \lambda^{p-1}-A_{2} \lambda^{p-2}-\cdots-A_{p}\right)=0$$
Mais les statisticiens préfèrent que les exposants soient identiques aux indices, ce qui revient à poser $$\lambda=\frac{1}{z} $$
ce qui donne:
$$\operatorname{det}\left(Id-A_{1} z-A_{2} z^{2}-\cdots-A_{p} z^{p}\right)=0$$
les solutions de cettes équations devant vérifier $|z|>1$ .

## Tests de stationnarité  

Au vu de ce qui précède, si une au moins des variables incluses dans le VAR n'est pas stationnaire, le VAR ne le sera pas. Donc il va s'agir de contrôler la stationnarité de chaque variable qu'on veut inclure  dans le modèle VAR.
Avant d'aborder les tests de stationnarité nous ferons un détour pour distiguer deux types de NON-stationnarité d'un processus univarié (ou d'une série univariée).

### non stationnarité déterministe
C'est le cas où le processus est de la forme:
$$X_{t}=f(t)+Z_{t}$$
où $Z_{t}$ est stationnaire et f fonction vectorielle. Pour stationnariser un tel processus, il suffit de lui soustraire la tendance déterministe. Cette tendance est souvent $f(t)=a+bt$ ou $f(t)=a+bt+ct^{2}$   
$X_{t}$ est appelé dans la littérature "trend-stationary process" ou plus simplement TS.


Nous allons développer un exemple.
Nous simulons le processus : 
$$X_{t}=0.7X_{t-1}+5+0.05t+\epsilon_{t}$$
Attention, il n'est pas a priori de la forme trend-stationary process. Nous en traçons une réalisation sur 100 valeurs, qui va suggerer un trend affine f(t), que nous allons estimer par régression de moindres carrés, et retrancher f(t) estimé au processus initial. Le résultat semble stationnaire, ce que nous vérifierons plus loin avec des tests adaptés.


```{r, trend stationary}
set.seed(123456)
B4<-matrix(c(5,0.05, 0.7),nrow=1)
vdat4=var(100,B=B4,lag=1,mod="trend")
ts.plot(vdat4,type="l",col=c(1))

lm=lm(vdat4~c(1:100))
lm
lines(lm$fitted.values,col="green")
lines(lm$residuals+15.6336,col="red")

```

### Non stationnarité stochastique

$X_{t}$ est caractérisé par le fait que le processus différencié $X_{t}-X_{t-1}$ est stationnaire.
$$(1-L) Y_{t}=Z_{t}\;stationnaire $$ 
Simulons l'exemple $Y{t}=Y_{t-1}+0.1+\epsilon_{t}$ (marche aléatoire avec dérive 0.3)

```{r, DS}
set.seed(123456)
B5<-matrix(c(0.3, 1),nrow=1)
vdat5=var(100,B=B5,lag=1,mod="const")
ts.plot(vdat5,type="l",col=c(1),ylim=c(-10,60),main="marche aléatoire et sa différentielle")

diff_vdat5=diff(vdat5,lag=1,differences=1)
lines(diff_vdat5,col="red")


```
Un tel processus est nommé processus DS (Difference stationnary), ou processus intégré d'ordre 1  ( I(1)).  
S'il faut différencier d fois avant la stationnarité on dit à ce moment là processus I(d).
Ce qui s'écrit:
$$(1-L)^{d} Y_{t}=Z_{t}$$
Le polynôme retard $(1-L)^d$ a 1 comme racine d'ordre d (ou d racines unitaires).

Il est à noter qu'on peut déceler la non stationnarité par le corrélogramme par une décroissance lente, mais sans que celà permette de distinguer DS ou TS. La stationnarité se signale

```{r, correlogramme}
acf(vdat4,main="vdat4")
acf(vdat5,main="vdat5")
acf(diff_vdat5,main="diff_vdat5")
```

### Test de Dickey-Fuller

L'hypothèse nulle H0 est la présence de racines unitaires (donc de NON stationnarité de type DS).
Le test va s'appliquer à une série univariée.
La fonction utilisée par R est "ur.df" de la librairie "urca".
L'existence de racines unitaires va être testée dans trois modèles différents:  

$$\Delta Y_{t}=\rho Y_{t-1}+\alpha+\beta t+\sum_{j=1}^{p} \phi_{j} \Delta Y_{t-j}+\varepsilon_{t}\;\;\;(modèle\:1)$$ 
$$\Delta Y_{t}=\rho Y_{t-1}+\alpha+\sum_{j=1}^{p} \phi_{j} \Delta Y_{t-j}+\varepsilon_{t}\;\;\;(modèle\:2)$$
$$\Delta Y_{t}=\rho Y_{t-1}+\sum_{j=1}^{p} \phi_{j} \Delta Y_{t-j}+\varepsilon_{t}\;\;\;(modèle\:3)$$
Il s'agit donc de vérifier la significativité des coefficients $\rho,\alpha,\beta$ comme avec le test de Student, mais avec des valeurs critiques différentes référencées dans les tables de Dickey-Fuller.  
L'hypothèse nulle H0 est; $\rho =0$ contre H1; $\rho <0$ , donc une zone de rejet à GAUCHE.
Pour $\alpha,\beta$ c'est leur valeur absolue qui est testée, donc zone de rejet à DROITE.


#### premier exemple
$$Y{t}=10+0.5t+0.7Y_{t-1}+\epsilon_{t}$$
série AR(1) avec trend et constante.

```{r, Dickey-Fuller 1}

set.seed(123456)
B2<-matrix(c(10,0.5,0.7),nrow=1)
vdat6=var(100,B=B2,lag=1,mod="trend")

summary(ur.df(vdat6,type="trend",lags=4))
```
Nous obtenons dans la colonne Estimate les valeurs estimées des coefficients avec leurs significativités signalées par les *** . Intercept et tt pour constante et trend, et z.lag.1 pour rho..les autres coeffients sont peu significatifs, ce qui implique qu'on refera le test avec lag=1. 

```{r, Dickey-Fuller 1.2}
summary(ur.df(vdat6,type="trend",lags=1))
```

Les conclusions sont irrévocables:  -6.1567 
donc H0 racine unitaire rejetée, 28.8 H01 racine unitaire ET trend nul rejeté, 19.56 H02 racine unitaie ET trend nul ET constante nulle rejeté. Il faut avouer que la lecture des coefficients nous donne une vision claire du modèle: pas de racine unitaire, un trend ET une constante significatifs, donc STATONNAIRE AUTOUR D'UN TREND.

Voyons le modèle 2 sans trend. Mais volontairement testons un modèle de type  avec trend.

$$Y{t}=5++0.7Y_{t-1}+\epsilon_{t}$$
```{r, Dickey-Fuller 2}
set.seed(123456)
B2<-matrix(c(5,0.7),nrow=1)
vdat7=var(100,B=B2,lag=1,mod="const")

summary(ur.df(vdat7,type="trend",lags=1))
```

tt n'a aucune significativité. On n'est donc pas dans le bon modèle, et on teste le modèle 2
 .
```{r, Dickey-Fuller 2.1} 
summary(ur.df(vdat7,type="drift",lags=1))
```

Et enfin pour être exhaustif nous simulons un processus avec racine unitaire ET constante ET trend.
$$Y{t}=10+0.5t+1Y_{t-1}+\epsilon_{t}$$


```{r, Dickey-Fuller 3}
set.seed(123456)
B2<-matrix(c(10,0.5,1),nrow=1)
vdat8=var(100,B=B2,lag=1,mod="trend")

summary(ur.df(vdat8,type="trend",lags=1))
```
H0 racine unitaire non rejetée, nullité des deux autres coefficients rejetée, donc racine unitaire avec trend et constante.  


## Estimation d'un modèle VAR

Nous allons commencer par étudier un processus simulé. Par exemple  VAR(2) sera notre base de données.

### première étape: s'assurer de la stationnarité des composantes.

```{r, test}
summary(ur.df(vardat2[,"y1"],type="drift",lags=1))
summary(ur.df(vardat2[,"y2"],type="drift",lags=1))
```

### recherche d'un décalage adapté.
```{r, recherche_lag}
lag=lags.select(vardat2,lag.max =4,include="const")
print(lag)
```

Nous choisissons un VAR(2)

### estimation du modèle.

```{r, estimation, message=FALSE,warning=FALSE}
library(vars)
varest2= VAR(vardat2,p = 2,type ="const",season = NULL,exogen = NULL)
print(varest2)
```
Nous y retrouvons une estimation correcte des coefficients du VAR(2) initial.

On peut contôler la stabilité du modèle estimé par la fonction "roots"
```{r, racines_unitaires}
roots=roots(varest2)
roots
```
### tests sur les résidus

D'abord tester l'absence d'autocorrélations sur les résidus (Test dit du "Portementeau") adapté aux séries multivariées ("serial.test")

```{r, portemanteau }
var.test2= serial.test(varest2,lags.pt =16,type ="PT.asymptotic")
var.test2
plot(var.test2,names="y1")
plot(var.test2,names="y2")

```
Puis test de normalité (test de Jarque-Bera).

```{r, normalité}
var.norm2=normality.test(varest2,multivariate.only = TRUE)
var.norm2
```

### Prédiction

Notre modèle VAR(2) une fois validé, nous pouvons l'utiliser pour la prédiction.
Nous fixons un horizon à 8, et traçons les prédictions suivant les 50 dernières valeurs ddes séries.
On observera aussi une petite fantaisie avec la fonction "fanchart".

```{r,  prediction}
pred2 = predict(varest2,n.ahead =8 ,ci = 0.95)
print(pred2)
plot(pred2,names="y1",xlim=c(450,508))
plot(pred2,names="y2",xlim=c(450,508))
fanchart(pred2,names = "y2",xlim=c(450,508))
```

### Causalités

Il nous faut expliquer dans un premier temps ce qu'on entend par causalité au sens de Granger.
En voici la définition:
Soit deux processus $\left(X_{t}\right)$ et $\left(Y_{t}\right)$, en notant $\underline{X}_{t}=\left\{X_{t}, X_{t-1}, \ldots\right\}$ et $\underline{Y}_{t}=\left\{Y_{t}, Y_{t-1}, \ldots\right\}$ les passés de $\left(X_{t}\right)$ et de $\left(Y_{t}\right)$ ,on dit que $\left(Y_{t}\right)$ cause $\left(X_{t}\right)$ à la date t si et seulement si $$\mathbb{E}\left(X_{t} | \underline{X}_{t-1}, \underline{Y}_{t-1}\right) \neq \mathbb{E}\left(X_{t} | \underline{X}_{t-1}\right)$$  

Autrement dit, Y cause X si la connaissance du passé de Y  modifie  le comportement de $\(X_{t}\right)$ 
Il s'agit d'une causalité de long terme.

Le test de causalité va correspondre à un test de Wald sur les coefficients estimés. Pour déterminer si y2 cause y1 on teste la nullité des coefficients de y2 dans l'expression de y1.

Parallèlement on utilise la causalité instantannée.  

$\left(Y_{t}\right)$ cause instantanément $\left(X_{t}\right)$ à la date t si et seulement si $$\mathbb{E}\left(X_{t} | \underline{X}_{t-1}, \underline{Y}_{t-1}\right) \neq \mathbb{E}\left(X_{t} | \underline{X}_{t-1}, \underline{Y}_{t}\right)$$  

l'introduction de Y à l'instant t (présent) dans le membre de droite de l'inégalité modifie le comportement de X (connaissant les passés de X et Y)

```{r, causalité}
causal2=causality( varest2, cause = "y2")
causal2
```

Nous rejetons les deux hypothèses de non causalité, et concluons à une double causalité (Granger et instantannée).  
Cette dernière est due au fait que nous avons simulé un VAR(2) avec des bruits blancs corrélés.



### Fonction de réponse impulsionnelle

Nous allons préciser l'impact d'un choc sur une variable sur l'autre (et réciproquement) et suivre son évolution dans le temps. La méthode consiste à simuler la valeur 1 pour $\epsilon_{1t}$ et en représenter la conséquence sur y2 par exemple sur 10 périodes. La fonction irf permet de réaliser une seule impulsion (boot=F), ou de répéter plusieurs fois l'opération (boot=T). L'option impulse = F montre l'effet du choc $\epsilon_{1t}$ sur $\epsilon_{2t}$ , alors que impulse = T montre l'effet sur Y2.
```{r, réponse impulsionnelle}
imp_inst.y1=irf(varest2,impulse ="y1",response="y2",n.ahead=10,ortho=FALSE,cumulative=F,boot=T,seed=12345)
plot(imp_inst.y1)
imp_cum.y1=irf(varest2,impulse ="y1",response="y2",n.ahead=10,ortho=FALSE,cumulative=T,boot=T,seed=12345)
plot(imp_cum.y1)
imp_inst.y2=irf(varest2,impulse ="y2",response="y1",n.ahead=10,ortho=FALSE,cumulative=F,boot=T,seed=12345)
plot(imp_inst.y2)
imp_cum.y2=irf(varest2,impulse ="y2",response="y1",n.ahead=10,ortho=FALSE,cumulative=T,boot=T,seed=12345)
plot(imp_cum.y2)


```
L'exemple montre un effet de diminition su Y2 et inverse sur y1.

## Application sur 5 séries temporelles.

Nous tenterons d'appliquer le modèle VAR sur l'évolution des actions Total et Engie, du Cac_40, du cours du pétrole Brent (pétrole de la Mer du Nord) et de taux de change euro-dollar sur une année, en données journalières (septembre 2019 à septembre 2020). Du fait de l'épidémie surgie en début 2020 une rupture importante se produit début avril. L'étude commencera donc à cette période (2020-03-25).

```{r, load datas, echo=FALSE}
library(dplyr)
data=read.table(file="Action_Total_Brent.txt",sep="\t",header=T,dec=".")
head(data)
h=5 # horizon de predict
l=dim(data)[1]-h 
deb=136  # début des données utilisées: 2020-03-25
data.train=data[deb:l,]
plot.ts(data.train[,-1],col="blue")
data.test=data[(l+1):(l+5),]
m=colMeans(data.train[,-1])
s=apply(data.train[,-1],2,sd)
#data.train=as.data.frame(cbind(data.train$Date,data.frame(Date,scale(data.train[,-1],center=T,scale=T))))
data.train.CR=data.frame(data.train$Date,scale(data.train[,-1],center=T,scale=T))
colnames(data.train.CR)=c("Date",colnames(data.train[,-1]))
data.test.CR=data.frame(data.test[,1],scale(data.test[,-1],center=m,scale=s))
colnames(data.test.CR)=c("Date",colnames(data.test[,-1]))

```
Les 4 dernières séries ne sont certainement pas stationnaires, ce que confirment les test de Dickey-Fuller.
La première variable "Total" mérite un traitement à part, elle ne présente pas de trend évident, donc nous lui soumettrons le modèle "drift".


```{r, Dickey}
print("Total")
print(summary(ur.df(data.train.CR$Total,type="drift",lag=1)))
for(i in c("Engie","Cac_40" ,"Taux_Change","Brent")) 
{ print(i)
  print(summary(ur.df(data.train.CR[,i],type="trend",lag=2)))
}

```


Nous allons différencier les variables ( $\Delta X_{t}=X_{t}- X_{t-1}$), et tenter un modèle VAR sur ces nouvelles variables ("D.Total","D.Engie","D.Cac_40" ,"D.Taux_Change","D.Brent) , après avoir vérifié leur stationnarité.

```{r, diff, results='hide'}
Ddata.train=apply(data.train.CR[,-1],2,diff)
colnames(Ddata.train)=c("D.Total","D.Engie","D.Cac_40" ,"D.Taux_Change","D.Brent")

for(i in c("D.Total","D.Engie","D.Cac_40" ,"D.Taux_Change","D.Brent")) { 
    print(i)
    print(summary(ur.df(Ddata.train[,i],type="none",lag=1)))
}
plot.ts(Ddata.train)
```
A ce stade nous devons examiner la cointégration des séries.

## Cointégration

Quelques définitions d'abord.

### Série intégrée d'ordre d:
Une série chronologique est dite *intégrée d'ordre d* si d est le plus petit entier tel que $\Delta^{d} X_{t}$ est stationnaire.

Les quatre séries de notre exemple son intégrées d'ordre 1 (on écrit I(1) )

### Séries cointégrées d'ordre d:
Une famille de séries intégrées  intégrées d'ordre d sont dites cointégrées d'ordre  C(d,0) ssi  il existe une combinaison linéaire de ces séries qui soit stationnaire.

Le test de Johansen permet de déterminer l'éventuelle cointégration.

Avant de procéder au test montrons le rôle que joue cette cointégration dans le modèle VAR.

On se place dans le cas de N variables intégrées I(1), et pour faciliter la compréhension de notre démonstration, nous n'envisageons qu'un VAR(2), mais la généralisation à l'ordre p ne fait aucune difficulté.  
Puisque nos variables sont I(1), leurs différentielles sont stationnaires, et donc bonnes candidates pour un VAR.  
Partons de l'expression VAR.

$$X_{t}=A_{0}+A_{1} X_{t-1}+A_{2} X_{t-2}+\varepsilon_{t}$$
D'où, en retranchant $X_{t-1}$ aux deux memebres de l'égalité. 
$$X_{t}-X_{t-1}=A_{0}+\left(A_{1}-I\right) X_{t-1}+A_{2} X_{t-2}+\varepsilon_{t}$$
$$\Longleftrightarrow \Delta X_{t}=A_{0}+\left(A_{1}-I\right)\left(X_{t-1}-X_{t-2}\right)+\left(A_{2}+A_{1}-I\right) X_{t-2}+\varepsilon_{t}$$
En utilisant maintenant $X_{t-2}$  
$$\Delta Y_{t}=\mu+(A_{1}-Id)( Y_{t-1}-Y_{t-2})+(A_{1}-Id)Y_{t-2}+A_{2} Y_{t-2}+u_{t}$$ 
Enfin;
$$\Longleftrightarrow \Delta X_{t}=A_{0}+\left(A_{1}-I\right) \Delta X_{t-1}+\left(A_{2}+A_{1}-I\right) X_{t-2}+\varepsilon_{t}$$
Là on retranche et rajoute $\left(A_{2}+A_{1}-I\right) X_{t-1} $
$$\begin{aligned}
\Delta X_{t}=& A_{0}+\left(A_{1}-I\right) \Delta X_{t-1}+\left(A_{2}+A_{1}-I\right) X_{t-2}
-\left(A_{2}+A_{1}-I\right) X_{t-1}+\left(A_{2}+A_{1}-I\right) X_{t-1}+\varepsilon_{t}
\end{aligned}$$
Pour obtenir
$$\Delta X_{t}=A_{0}-A_{2} \Delta X_{t-1}+\left(A_{1}+A_{2}-I\right) X_{t-1}+\varepsilon_{t}$$
Ou en renommant les matrices
$$\Delta X_{t}=B_{0}+B_{1} \Delta X_{t-1}+\Pi X_{t-1}+\varepsilon_{t}$$
A ce stade plusieurs cas vont se présenter suivant le rang de la matrice $\Pi$ 

* $rang(\Pi)=0$ La matrice est nulle, donc le VAR sur les séries différenciées se déroule normalement.
* $rang(\Pi)=N$ La matrice est inversible, donc 

$$X_{t-1}=\Pi^{-1}\left(\Delta X_{t}-B_{0}-B_{1} \Delta X_{t-1}-\epsilon_{t}\right)$$ 
$X_{t-1}$ serait égal à une transformation linéaire de variables stationnaires, donc serait elle même stationnaire, ce qui n'est pas vrai dans notre contexte.

* $rang(\Pi)=1$ 
Dans ce cas l'image de $\Pi$ est de dimension 1 engendrée par un vecteur u. Une bonne connaisssance de l'algèbre linéaire permet de démontrer que $\Pi=u.l$ où l est un vecteur ligne $l=(l_{1},l_{2},...l_{N})$
Tous calculs faits (faites les !!) on a $\Pi X_{t-1}=\left (\sum l_{i}X_{i_{t-1}}\right)u$ qu'on peut écrire
$\lambda_{t-1}u$ avec $\lambda_{t-1}$ processus univarié stationnaire (la stationnarité de $\lambda_{t-1}$ résulte de l'écriture du VAR sous la forme  
$\Delta Engle,R.E. & Granger,C.W.J. (1987). ’Cointegration and error-correction : representation, estimation
and testing’. Econometrica. 55=B_{0}+B_{1} \Delta X_{t-1}+\lambda_{t-1}u+\varepsilon_{t}$ )
De plus $\sum l_{i}X_{i_{t-1}}$ étant stationnaire, on obtient une relation de cointégration des variables.


Dans ce cas $\Pi X_{t-1}$ est un vecteur de N coordonnées proportionnelles du type $\lambda_{t-1}u$ u vecteur constant de N coordonnées et $\lambda_{t-1}$ processus univarié stationnaire (la stationnarité de $\lambda_{t-1}$ résulte de l'écriture du VAR sous la forme  
$\Delta X_{t}=B_{0}+B_{1} \Delta X_{t-1}+\lambda_{t-1}u+\varepsilon_{t}$ )
On voit alors que chaque coordonnée j de $\Pi X_{t-1}$ est $\sum_{i}a_{i}X_{i_{t-1}}=\lambda_{t-1}u_{j}$
Ce qui indique que les $ X_{i_{t}}$ sont cointégrés d'ordre 1. 
En plus on peut réécrire $\Pi X_{t-1}$ sous la forme $u^{'}AX_{t-1}$ où A est la matrice-colonne des coefficients de la relation de cointégration. u est appelé force de rappel vers l'équilibre de long terme.

* $rang(\Pi)=r$ 

Granger montre (Engle,R.E. & Granger,C.W.J. (1987). ’Cointegration and error-correction : representation, estimation
and testing’. Econometrica. 55 ) que dans ce cas il existe deux matrices de rang r $U$ et $V$ telles que $\Pi=U.V^{'}$ telles que $V^{'}X_{t}$ est stationnaire, chaque colonne de V étant un vecteur de cointégration. (il y a donc r rlations de cointégration).


```{r, test Johansen}
test=ca.jo(data.train.CR[,c(-1)], type="trace", K=3, ecdet="none", spec="longrun")
summary(test)

```
Il apparaît qu'avec 5 variables  r=0 n'est est pas rejeté

## Modèle VAR sur les cinq variables

### recherche d'un décalage adapté.
```{r, recherche_lag2}
Def=Ddata.train
lag=lags.select(Def,lag.max =4,include="none")
print(lag)
```
Nous retenons un VAR(1).

## Estimation du modèle, racines unitaires et test sur les résidus, y compris sur leur normalité par le test de Jarque-Bera

```{r, modèle}
varModel= VAR(Def,p = 1,type ="none",season = NULL,exogen = NULL)
print(varModel)
roots=roots(varModel)
roots
varModel.test= serial.test(varModel,lags.pt =16,type ="PT.asymptotic")
varModel.test
plot(varModel.test,names="D.Total")
plot(varModel.test,names="D.Engie")
plot(varModel.test,names="D.Cac_40")
plot(varModel.test,names="D.Taux_Change")
plot(varModel.test,names="D.Brent")
normality.test(varModel,multivariate.only = TRUE)

```
La normalité de résidus semble convenable (sauf pour "Total"), et surtout l'absence d'auto corrélation est acquise.


```{r, causality}
causal.varModel=causality( varModel, cause =c("Cac_40","Taux_Change") )
causal.varModel

causal.varModel=causality( varModel, cause =c("Total","Taux_Change") )
causal.varModel

causal.varModel=causality( varModel, cause =c("Cac_40","Total") )
causal.varModel


```

Seule une causalité instantannée des accroissements des deux variables "Cac_40","Taux_Change" sur les accroissements "Total" est à retenir.

```{r, réponse impulsionnelle Total}
imp_inst.Total=irf(varModel,impulse ="Cac_40",response="Total",n.ahead=10,ortho=FALSE,cumulative=F,boot=T,seed=12345)
plot(imp_inst.Total)
imp.Total2=irf(varModel,impulse ="Taux_Change",response="Total",n.ahead=10,ortho=FALSE,cumulative=T,boot=T,seed=12345)
plot(imp.Total2)
imp_inst.Cac_40=irf(varModel,impulse ="Total",response="Cac_40",n.ahead=10,ortho=FALSE,cumulative=F,boot=T,seed=12345)
plot(imp_inst.Cac_40)

cor(Ddata.train$Total,Ddata.train$Cac_40)  # ????

```




